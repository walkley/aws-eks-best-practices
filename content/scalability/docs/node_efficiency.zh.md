---
date: 2023-09-22
authors:
  - Shane Corbett
---
# 节点和工作负载效率
提高工作负载和节点的效率可以降低复杂性/成本，同时提高性能和扩展能力。在规划效率时需要考虑许多因素，从权衡取舍的角度思考比为每个功能设置一个最佳实践更容易。让我们在下一节深入探讨这些权衡。

## 节点选择
使用稍大的节点大小(4-12xlarge)可以增加运行 pod 的可用空间，因为它减少了用于"开销"的节点百分比，例如 [DaemonSets](https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/) 和为系统组件保留的 [Reserves](https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/)。在下图中，我们看到了在具有适度数量 DaemonSets 的情况下，2xlarge 与 8xlarge 系统的可用空间之间的差异。

!!! note
    由于 k8s 通常是水平扩展，因此对于大多数应用程序，采用 NUMA 大小节点的性能影响是没有意义的，因此建议采用下面的节点大小范围。

![节点大小](../images/node-size.png)

较大的节点大小使我们能够获得更高百分比的每个节点可用空间。但是，如果节点上打包了太多 pod 导致错误或节点饱和，这种模式就会走向极端。成功使用较大节点大小的关键是监控节点饱和情况。

节点选择很少是一刀切的。通常最好将周转率极端不同的工作负载分割到不同的节点组。小批量工作负载具有较高的周转率，最适合 4xlarge 系列实例，而像 Kafka 这样大规模应用程序占用 8 个 vCPU 且周转率较低，最适合 12xlarge 系列。

![周转率](../images/churn-rate.png)

!!! tip
    对于非常大的节点大小，另一个需要考虑的因素是，由于CGROUPS不会隐藏容器化应用程序的总vCPU数量。动态运行时经常会产生意外数量的OS线程，从而导致难以排查的延迟。对于这些应用程序，[CPU绑定](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#static-policy)是推荐的做法。要深入探讨这个主题，请观看以下视频https://www.youtube.com/watch?v=NqtfDy_KAqg

## 节点装箱
### Kubernetes与Linux规则
在处理Kubernetes上的工作负载时，我们需要注意两组规则。Kubernetes调度器使用请求值在节点上调度Pod的规则，以及Pod被调度后发生的情况，这属于Linux的领域，而不是Kubernetes。

在Kubernetes调度器完成后，一组新的规则就会生效，即Linux完全公平调度器(CFS)。关键要点是Linux CFS没有核心的概念。我们将讨论为什么以核心思考会导致大规模优化工作负载的重大问题。

### 以核心思考
混淆的起源是因为Kubernetes调度器确实有核心的概念。从Kubernetes调度器的角度来看，如果我们看一个节点有4个NGINX Pod，每个Pod请求一个核心，那么节点看起来就是这样的。

![](../images/cores-1.png)

然而，让我们从Linux CFS的角度做一个思考实验。使用Linux CFS系统时最重要的是要记住:只有繁忙的容器(CGROUPS)才会计入共享系统。在这种情况下，只有第一个容器很忙，所以它被允许使用节点上的所有4个核心。

![](../images/cores-2.png)

为什么这很重要呢?假设我们在开发集群中进行了性能测试，在该节点上，NGINX应用程序是唯一繁忙的容器。当我们将应用程序移至生产环境时，会发生以下情况:NGINX应用程序需要4个vCPU资源，但是由于节点上的所有其他 pod 都很繁忙，我们的应用程序性能受到限制。

![](../images/cores-3.png)

这种情况会导致我们不必要地添加更多容器，因为我们没有允许应用程序扩展到其"最佳点"。让我们更详细地探讨一下这个重要的"最佳点"概念。

### 应用程序适当调整大小
每个应用程序都有一个无法承受更多流量的点。超过这个点会增加处理时间，甚至在远远超过这个点时丢弃流量。这被称为应用程序的饱和点。为了避免扩展问题，我们应该在应用程序达到饱和点之前尝试扩展应用程序。让我们称这个点为最佳点。

![最佳点](../images/sweet-spot.png)

我们需要测试每个应用程序以了解其最佳点。这里没有通用指导，因为每个应用程序都不同。在此测试过程中，我们试图了解最能显示应用程序饱和点的最佳指标。通常使用利用率指标来指示应用程序已饱和，但这很快会导致扩展问题(我们将在后面的部分详细探讨这个主题)。一旦我们有了这个"最佳点",我们就可以有效地扩展我们的工作负载。

相反，如果我们在最佳点之前就大规模扩展并创建了不必要的 pod，会发生什么情况?让我们在下一节中探讨这个问题。

### Pod 扩散
为了了解创建不必要的 Pod 如何很快失控，让我们看看左边的第一个示例。当处理每秒 100 个请求时，此容器的正确垂直比例大约占用 2 个 vCPU 的利用率。但是，如果我们将 requests 值设置为一半核心而低估了，我们现在需要 4 个 Pod 才能满足实际需要的 1 个 Pod。进一步加剧这个问题，如果我们的 [HPA](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) 设置为默认的 50% CPU,那些 Pod 将以一半的空闲状态进行扩展，创建 8:1 的比率。

![](../images/scaling-ratio.png)

扩大这个问题，我们很快就能看到它是如何失控的。如果十个 Pod 的部署的最佳点设置不正确，很快就会蔓延到 80 个 Pod 以及运行它们所需的额外基础设施。

![](../images/bad-sweetspot.png)

现在我们了解了不允许应用程序在其最佳点运行的影响，让我们回到节点级别，问一下 Kubernetes 调度器和 Linux CFS 之间的这种差异为什么如此重要?

在使用 HPA 进行扩缩容时，我们可能会遇到有大量空间可以分配更多 Pod 的情况。这将是一个糟糕的决定，因为左边所示的节点已经达到 100% 的 CPU 利用率。在一个不切实际但理论上可能的极端情况下，我们的节点可能完全满载，但 CPU 利用率为零。

![](../images/hpa-utilization.png)
### 设置 Requests
将 request 设置为该应用程序的"最佳点"值似乎很诱人，但这会导致下图所示的低效。在这里，我们将 request 值设置为 2 个 vCPU，但这些 Pod 的平均利用率大多数时候只有 1 个 CPU。这种设置会导致我们浪费 50% 的 CPU 周期，这是不可接受的。

![](../images/requests-1.png)

这让我们来看复杂问题的答案。容器利用率不能在真空中考虑;必须考虑节点上运行的其他应用程序。在下面的示例中，具有爆发性的容器与两个低CPU利用率的容器混合在一起，后者可能受到内存限制。通过这种方式，我们允许容器达到其最佳状态，而不会加重节点的负担。

![](../images/requests-2.png)

从所有这些中需要领会的重要概念是，使用Kubernetes调度程序的核心概念来理解Linux容器性能可能会导致决策失误，因为它们无关。

!!! tip
    Linux CFS确实有其优点。这对于基于I/O的工作负载尤其如此。但是，如果您的应用程序使用全核而没有sidecar，并且没有I/O要求，CPU固定可以大大简化此过程，并鼓励在这些警告下使用。

## 利用率与饱和度
应用程序扩展中的一个常见错误是仅使用CPU利用率作为扩展指标。在复杂的应用程序中，这几乎总是一个糟糕的指标，表明应用程序实际上已经饱和了请求。在左侧的示例中，我们看到所有请求实际上都击中了Web服务器，因此CPU利用率与饱和度一致。

在真实世界的应用程序中，很可能有一些请求将由数据库层或身份验证层等服务。在这种更常见的情况下，请注意CPU并没有跟踪饱和度，因为请求正在由其他实体服务。在这种情况下，CPU是一个非常糟糕的饱和度指标。

![](../images/util-vs-saturation-1.png)

在应用程序性能中使用错误的指标是导致Kubernetes中不必要和不可预测扩缩容的头号原因。在选择正确的饱和度指标时，必须非常小心，因为这取决于您使用的应用程序类型。重要的是要注意，没有一种放之四海而皆准的建议。根据所使用的语言和所讨论的应用程序类型，有各种各样的饱和度指标。

我们可能认为这个问题只存在于CPU利用率，但其他常见的指标，如每秒请求数也可能遇到与上述相同的问题。请注意，请求也可能会发送到数据库层、身份验证层，而不是直接由我们的Web服务器处理，因此它是Web服务器本身真正饱和度的一个糟糕指标。

![](../images/util-vs-saturation-2.png)

不幸的是，在选择正确的饱和度指标时，没有简单的答案。以下是一些需要考虑的指导原则:

* 了解您的语言运行时 - 使用多个操作系统线程的语言与单线程应用程序的反应不同，因此对节点的影响也不同。
* 了解正确的垂直扩展 - 在扩展新的 Pod 之前，您希望在应用程序的垂直扩展中保留多少缓冲区?
* 哪些指标真正反映了您的应用程序的饱和度 - Kafka 生产者的饱和度指标与复杂的 Web 应用程序会有很大不同。
* 节点上的其他应用程序如何相互影响 - 应用程序性能不是在真空中进行的，节点上的其他工作负载会产生重大影响。

最后总结一下这一部分，很容易将上述内容视为过于复杂和不必要。通常情况下，我们可能会遇到问题，但由于我们查看的指标错误，因此无法了解问题的真正本质。在下一节中，我们将看到这种情况是如何发生的。

### 节点饱和
现在我们已经探讨了应用程序饱和，让我们从节点的角度来看这个概念。让我们看看两个 100% 利用率的 CPU 在利用率和饱和之间的区别。

左边的 vCPU 100% 被利用，但没有其他任务在等待运行在这个 vCPU 上，所以从理论上讲，这是相当有效率的。与此同时，在第二个示例中，我们有 20 个单线程应用程序正在等待被 vCPU 处理。现在所有 20 个应用程序在等待轮到被 vCPU 处理时都会经历某种类型的延迟。换句话说，右边的 vCPU 已经饱和。

如果我们只看利用率，不仅我们不会看到这个问题，而且我们可能会将这种延迟归因于与网络无关的其他原因，这会导致我们走错路。

![](../images/node-saturation.png)

在任何给定时间增加在节点上运行的 pod 总数时，查看饱和度指标而不仅仅是利用率指标很重要，因为我们很容易忽略了节点过度饱和的事实。对于这个任务，我们可以使用下图中显示的压力停滞信息指标。

PromQL - 停滞的 I/O

```
topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
```

![](../images/stalled-io.png)

!!! note
    有关压力停滞指标的更多信息，请参阅 https://facebookmicrosites.github.io/psi/docs/overview*

通过这些指标，我们可以知道线程是否在等待 CPU，甚至每个线程在该实例上是否都在等待诸如内存或 I/O 之类的资源而停滞。例如，我们可以看到在 1 分钟的时间段内，每个线程在实例上等待 I/O 的百分比。

```
topk(3, ((irate(node_pressure_io_stalled_seconds_total[1m])) * 100))
```

使用这个指标，我们可以在上图中看到，在高峰时期，盒子上的每个线程都有45%的时间在等待I/O而被阻塞，这意味着我们在那一分钟内浪费了所有这些CPU周期。了解这一点发生可以帮助我们重新获得大量的vCPU时间，从而使扩展更加高效。

### HPA V2
建议使用autoscaling/v2版本的HPA API。HPA API的旧版本在某些边缘情况下可能会陷入扩展困境。它还限制了pod在每个扩展步骤中只能加倍，这为需要快速扩展的小型部署带来了问题。

Autoscaling/v2允许我们更灵活地包含多个标准进行扩展，并在使用自定义和外部指标(非K8s指标)时为我们提供了更大的灵活性。

例如，我们可以根据三个值中的最高值进行扩展(见下文)。如果所有pod的平均利用率超过50%,如果自定义指标ingress的每秒数据包超过平均1，000个，或者ingress对象超过每秒10K个请求，我们就会扩展。

!!! note
    这只是为了展示自动缩放API的灵活性，我们不建议使用过于复杂的规则，因为这些规则在生产环境中很难排查故障。

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: php-apache
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: php-apache
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k
  - type: Object
    object:
      metric:
        name: requests-per-second
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: main-route
      target:
        type: Value
        value: 10k
```

然而，我们了解到在复杂的Web应用程序中使用这种指标的危险性。在这种情况下，使用能够准确反映我们的应用程序饱和度与利用率的自定义或外部指标会更好。HPAv2通过能够根据任何指标进行扩缩容来实现这一点，但我们仍需要找到并将该指标导出到Kubernetes以供使用。

例如，我们可以查看Apache中的活动线程队列计数。这通常会创建一个"更平滑"的扩缩容配置文件(稍后将详细介绍这个术语)。如果一个线程处于活动状态，无论该线程是在等待数据库层还是在本地服务请求，如果应用程序的所有线程都在被使用，这就是应用程序已饱和的一个很好的指示。

我们可以使用这种线程耗尽作为一个信号来创建一个新的Pod，其线程池完全可用。这也使我们能够控制在高流量时期我们希望在应用程序中保留多大的缓冲区。例如，如果我们总共有10个线程池，在4个线程使用时扩缩容与在8个线程使用时扩缩容将对我们在扩缩容应用程序时可用的缓冲区产生重大影响。设置为4将适用于需要在高负载下快速扩缩容的应用程序，而设置为8则在我们有足够时间扩缩容的情况下(由于请求数量随时间缓慢而非急剧增加)更加高效地利用资源。

![](../images/thread-pool.png)

我们所说的"平滑"扩缩容是什么意思?请注意下图，我们使用CPU作为指标。此部署中的Pod在短时间内从50个Pod一直激增到250个Pod，然后又立即缩减。这种高度低效的扩缩容是集群中抖动的主要原因。

![](../images/spiky-scaling.png)

注意到在我们将指标更改为反映我们应用程序正确的最佳点（图表的中间部分）后，我们能够平滑地进行扩缩。我们的扩缩现在是高效的，我们的 pod 被允许通过调整请求设置提供的余量来完全扩缩。现在，一小组 pod 正在做之前数百个 pod 所做的工作。实际数据显示，这是 Kubernetes 集群可扩展性的第一个因素。

![](../images/smooth-scaling.png)

关键要点是 CPU 利用率只是应用程序和节点性能的一个维度。将 CPU 利用率作为我们节点和应用程序的唯一健康指标会在扩缩、性能和成本方面造成问题，这些概念都是紧密相连的。应用程序和节点的性能越好，需要扩缩的程度就越小，从而降低了成本。

找到并使用正确的饱和指标来扩缩您的特定应用程序，还允许您监控和报警该应用程序的真正瓶颈。如果跳过了这一关键步骤，就很难甚至不可能理解性能问题报告。

## 设置 CPU 限制
为了总结这一节关于被误解的主题，我们将介绍 CPU 限制。简而言之，限制是与容器相关联的元数据，它有一个每 100 毫秒重置一次的计数器。这有助于 Linux 跟踪特定容器在 100 毫秒的时间段内在节点范围内使用了多少 CPU 资源。

![CPU limits](../images/cpu-limits.png)

设置限制时的一个常见错误是假设应用程序是单线程的，并且只在其"分配的" vCPU 上运行。在上一节中，我们了解到 CFS 不会分配内核，实际上，运行大型线程池的容器将在该盒子上的所有可用 vCPU 上调度。

如果64个操作系统线程跨64个可用内核运行(从Linux节点的角度来看),在所有这64个内核上运行的时间加起来后，我们将在100毫秒的时间段内产生相当大的总CPU使用费用。由于这可能只会在垃圾收集过程中发生，因此很容易错过这种情况。这就是为什么在尝试设置限制之前，有必要使用指标来确保我们随时间的正确使用情况。

幸运的是，我们有一种方法可以准确看到应用程序中所有线程正在使用多少vCPU。我们将使用指标`container_cpu_usage_seconds_total`来实现这一目的。

由于节流逻辑每100毫秒发生一次，而这个指标是每秒指标，我们将使用PromQL来匹配这个100毫秒的时间段。如果您想深入了解这个PromQL语句的工作原理，请参阅以下[博客](https://aws.amazon.com/blogs/containers/using-prometheus-to-avoid-disasters-with-kubernetes-cpu-limits/)。

PromQL查询:

```
topk(3, max by (pod, container)(rate(container_cpu_usage_seconds_total{image!="", instance="$instance"}[$__rate_interval]))) / 10
```

![](../images/cpu-1.png)

一旦我们觉得有了正确的值，我们就可以在生产环境中设置限制。然后有必要查看我们的应用程序是否由于某些意外原因而被节流。我们可以通过查看`container_cpu_throttled_seconds_total`来做到这一点

```
topk(3, max by (pod, container)(rate(container_cpu_cfs_throttled_seconds_total{image!=``""``, instance=``"$instance"``}[$__rate_interval]))) / 10
```

![](../images/cpu-2.png)
### 内存
内存分配是另一个容易将Kubernetes调度行为与Linux CGroup行为混淆的例子。这是一个更微妙的话题，因为CGroup v2在Linux中处理内存的方式发生了重大变化，Kubernetes也相应地改变了其语法;阅读这篇[博客](https://kubernetes.io/blog/2021/11/26/qos-memory-resources/)以了解更多细节。

与CPU请求不同，内存请求在调度过程完成后会被闲置。这是因为我们无法像CPU那样在CGroup v1中压缩内存。这使我们只剩下内存限制，它旨在通过完全终止pod来防止内存泄漏。然而，这是一种非此即彼的方式，但现在我们有了新的方法来解决这个问题。

首先，重要的是要理解为容器设置正确的内存量并不像看上去那么简单。Linux中的文件系统会将内存用作缓存以提高性能。这个缓存会随着时间的推移而增长，很难知道有多少内存只是为了缓存而很好，但可以在不显著影响应用程序性能的情况下被回收。这通常会导致对内存使用的误解。

能够"压缩"内存是推动CGroup v2的主要原因之一。要了解更多关于为什么需要CGroup V2的历史，请参阅Chris Down在LISA21上的[演讲](https://www.youtube.com/watch?v=kPMZYoRxtmg),他在其中讲述了无法正确设置最小内存是促使他创建CGroup v2和压力停滞指标的原因之一。

值得庆幸的是，Kubernetes现在在`requests.memory`下有了`memory.min`和`memory.high`的概念。这为我们提供了积极释放此缓存内存供其他容器使用的选项。一旦容器达到内存高限制，内核就可以积极回收该容器的内存，直到达到`memory.min`设置的值。因此，当节点内存压力较大时，为我们提供了更大的灵活性。

关键问题是，将 `memory.min` 设置为多少值？这就是内存压力停滞指标发挥作用的地方。我们可以使用这些指标来检测容器级别的内存"抖动"。然后我们可以使用诸如 [fbtax](https://facebookmicrosites.github.io/cgroup2/docs/fbtax-results.html) 这样的控制器来检测 `memory.min` 的正确值，通过查找这种内存抖动，并动态设置 `memory.min` 值为此设置。

### 总结
总结一下本节内容，很容易混淆以下概念:

* 利用率和饱和度
* Linux 性能规则与 Kubernetes 调度程序逻辑

必须非常小心地区分这些概念。性能和规模在深层次上是相互关联的。不必要的扩展会导致性能问题，而性能问题又会导致扩展问题。