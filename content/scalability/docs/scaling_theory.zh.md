---
date: 2023-09-22
authors: 
  - Shane Corbett
---
# Kubernetes 扩缩容理论

## 节点与变更率
通常在讨论 Kubernetes 的可扩展性时，我们会以单个集群中节点的数量来衡量。有趣的是，这很少是理解可扩展性的最有用指标。例如，一个拥有 5，000 个节点但 Pod 数量较大但固定的集群，在初始设置之后不会给控制平面带来太大压力。但是，如果我们在不到一分钟的时间内尝试在 1，000 个节点的集群上创建 10，000 个短暂的作业，那将给控制平面带来巨大的持续压力。

仅使用节点数量来理解扩缩容可能会产生误导。更好的方式是以特定时间段内发生的变更率来思考(让我们在此讨论中使用 5 分钟的时间间隔，因为这是 Prometheus 查询默认使用的时间间隔)。让我们探讨一下，以变更率的角度来框定这个问题，为什么能让我们更好地了解需要调整哪些参数以实现所需的扩缩容。

## 以每秒查询数思考
Kubernetes 为每个组件 - Kubelet、Scheduler、Kube Controller Manager 和 API Server - 都有一些保护机制，以防止下一个 Kubernetes 链路被压垮。例如，Kubelet 有一个标志可以限制对 API Server 的调用速率。这些保护机制通常(但不总是)以每秒允许的查询数或 QPS 来表示。

在更改这些 QPS 设置时必须格外小心。消除一个瓶颈，比如 Kubelet 的每秒查询数，将会影响下游的其他组件。这可能并且确实会在一定速率之上压垮整个系统，因此了解和监控服务链条中的每一个部分都是成功扩缩容 Kubernetes 工作负载的关键。

!!! note 
    API Server 有一个更复杂的系统，引入了 API 优先级和公平性，我们将单独讨论。

!!! note
    注意，有些指标看似合适，但实际上是在测量其他内容。例如，`kubelet_http_inflight_requests`与Kubelet中的指标服务器有关，而不是从Kubelet到apiserver的请求数量。这可能会导致我们错误地配置Kubelet上的QPS标志。查询特定Kubelet的审计日志将是检查指标的更可靠方式。

## 扩展分布式组件
由于EKS是一项托管服务，让我们将Kubernetes组件分为两类:AWS托管组件，包括etcd、Kube控制器管理器和调度程序(位于图表左侧),以及客户可配置的组件，如Kubelet、容器运行时和调用AWS API的各种操作符，如网络和存储驱动程序(位于图表右侧)。我们将API服务器置于中间，尽管它是由AWS管理的，但客户可以配置API优先级和公平性设置。

![Kubernetes组件](../images/k8s-components.png)

## 上游和下游瓶颈
在监控每个服务时，查看双向指标以寻找瓶颈非常重要。让我们以Kubelet为例，了解如何做到这一点。Kubelet与API服务器和容器运行时进行通信;我们需要监控**什么**和**如何**来检测任一组件是否出现问题?

### 每个节点的Pod数量
当我们查看扩展数字时，例如一个节点可以运行多少个pod，我们可以直接采用上游支持的110个pod/节点的数值。

!!! note
    https://kubernetes.io/docs/setup/best-practices/cluster-large/

但是，您的工作负载可能比在上游可扩展性测试中测试的更复杂。为了确保我们可以在生产环境中运行所需的pod数量，让我们确保Kubelet能够"跟上"Containerd运行时。

![跟上](../images/keeping-up.png)

过于简单化的说，Kubelet 正在从容器运行时(在我们的例子中是 Containerd)获取 pod 的状态。如果有太多 pod 状态发生太快的变化会怎样?如果变化率太高，请求[容器运行时]可能会超时。

!!! note
    Kubernetes 不断发展，这个子系统目前正在进行变更。https://github.com/kubernetes/enhancements/issues/3386

![Flow](../images/flow.png)
![PLEG duration](../images/PLEG-duration.png)

在上面的图表中，我们看到一条平坦的线，表示我们刚刚达到了 pod 生命周期事件生成持续时间指标的超时值。如果您想在自己的集群中查看这一点，可以使用以下 PromQL 语法。

```
increase(kubelet_pleg_relist_duration_seconds_bucket{instance="$instance"}[$__rate_interval])
```

如果我们目睹了这种超时行为，我们就知道我们已经超过了节点所能承受的限度。在继续之前，我们需要先解决导致超时的原因。这可以通过减少每个节点上的 pod 数量来实现，或者寻找可能导致大量重试(从而影响变化率)的错误。重要的是要认识到，与使用固定数量相比，指标是了解节点是否能够处理分配的 pod 变化率的最佳方式。

## 根据指标进行扩展
虽然使用指标来优化系统的概念已经存在很长时间，但在人们开始使用 Kubernetes 的过程中，这一点往往被忽视了。我们不再关注特定的数字(例如每个节点 110 个 pod),而是将精力集中在寻找帮助我们发现系统瓶颈的指标上。了解这些指标的正确阈值可以让我们高度确信我们的系统配置是最优的。

### 变更的影响
一个可能会给我们带来麻烦的常见模式是，专注于第一个看起来可疑的指标或日志错误。当我们之前看到Kubelet超时时，我们可能会尝试一些随机的事情，比如增加Kubelet每秒允许发送的速率等。然而，明智的做法是先全面查看错误下游的整个情况。*每次变更都要有目的，并有数据支持*。

Kubelet下游是Containerd运行时(pod错误)、存储驱动(CSI)和网络驱动(CNI)等DaemonSet，它们与EC2 API通信等。

![Flow add-ons](../images/flow-addons.png)

让我们继续之前Kubelet无法跟上运行时的示例。有许多情况下，我们可能会将节点打包得如此密集，以至于触发错误。

![Bottlenecks](../images/bottlenecks.png)

在为我们的工作负载设计合适的节点大小时，这些很容易被忽视的信号可能会给系统带来不必要的压力，从而限制了我们的扩展和性能。

### 不必要错误的代价

当出现错误情况时，Kubernetes控制器擅长重试，但这是有代价的。这些重试可能会增加对Kube Controller Manager等组件的压力。监控此类错误是规模测试的一个重要原则。

当发生的错误较少时，就更容易发现系统中的问题。通过在重大操作(如升级)之前定期确保集群没有错误，我们可以简化在发生意外事件时的故障排除日志。

#### 扩展我们的视野

在大规模集群中拥有数千个节点时，我们不希望逐个查找瓶颈。在 PromQL 中，我们可以使用名为 topk 的函数找到数据集中的最高值;K 是我们放置所需项目数量的变量。这里我们使用三个节点来了解集群中的所有 Kubelet 是否都已饱和。到目前为止，我们一直在关注延迟，现在让我们看看 Kubelet 是否正在丢弃事件。

```
topk(3, increase(kubelet_pleg_discard_events{}[$__rate_interval]))
```

分解这个语句。

* 我们使用 Grafana 变量 `$__rate_interval` 来确保它获得所需的四个样本。这绕过了一个复杂的监控主题，使用了一个简单的变量。
* `topk` 只给出最高结果，数字 3 将这些结果限制为三个。这对于集群范围的指标是一个有用的函数。
* `{}` 告诉我们没有过滤器，通常你会放置抓取规则的作业名称，但由于这些名称会有所不同，我们将其留空。

#### 将问题一分为二

为了解决系统中的瓶颈，我们将采取寻找一个指标来显示上游或下游存在问题的方法，因为这允许我们将问题一分为二。这也将成为我们显示指标数据的核心原则之一。

开始这个过程的一个好地方是 API 服务器，因为它允许我们看到客户端应用程序或控制平面是否存在问题。